{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjL9QHP6vog3",
        "outputId": "c611188f-a3c0-46e9-ea6c-3968f50c566e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Waiting for headers] [Connected to r2u.stat.illinois\r                                                                                                    \rGet:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "\r                                                                                                    \rGet:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "\r                                                                                                    \rGet:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\r                                                                                                    \rHit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "\r0% [2 InRelease 47.5 kB/128 kB 37%] [3 InRelease 14.2 kB/129 kB 11%] [Waiting for headers] [Connecte\r0% [Waiting for headers] [3 InRelease 14.2 kB/129 kB 11%] [Waiting for headers] [Connected to ppa.la\r                                                                                                    \rGet:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,513 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,738 kB]\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,619 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [33.8 kB]\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,506 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,454 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,224 kB]\n",
            "Fetched 19.5 MB in 5s (3,993 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "# Find the latest version of spark 3.x  from http://www.apache.org/dist/spark/ and enter as the spark version\n",
        "# For example:\n",
        "# spark_version = 'spark-3.3.1'\n",
        "spark_version = 'spark-3.5.3'\n",
        "os.environ['SPARK_VERSION']=spark_version\n",
        "\n",
        "# Install Spark and Java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop3.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set Environment Variables\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop3\"\n",
        "\n",
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxLb0n8UBanX"
      },
      "outputs": [],
      "source": [
        "# Importing pyspark packages and linking google colab driver\n",
        "from pyspark.sql import functions as f\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7C_l-2PkrpN"
      },
      "outputs": [],
      "source": [
        "# Starting the spark session\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"GoogleDriveCSV\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Getting the correct file path\n",
        "file_path = '/content/drive/My Drive/Fraud_Detection/fraudTest.csv'\n",
        "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtdNXdpnn5JA"
      },
      "outputs": [],
      "source": [
        "# Finding the total rows in the first data set\n",
        "total_rows = df.count()\n",
        "print(total_rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VRF8Bw8krPL"
      },
      "outputs": [],
      "source": [
        "# Getting the second file path to prepair for union\n",
        "file_path2 = '/content/drive/My Drive/Fraud_Detection/fraudTrain.csv'\n",
        "train_df = spark.read.csv(file_path2, header=True, inferSchema=True)\n",
        "train_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anEvyejcn3R5"
      },
      "outputs": [],
      "source": [
        "# Finding the total rows in the second data set\n",
        "total_rows_2 = train_df.count()\n",
        "print(total_rows_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0ojm0V4mbbu"
      },
      "outputs": [],
      "source": [
        "# creating a union data set from both csv files and finding the total number of rows\n",
        "union_df = df.union(train_df)\n",
        "print(union_df.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FsTf8N-n8Zn1"
      },
      "outputs": [],
      "source": [
        "# Dropping columns that we found not important for the model\n",
        "clean_df = union_df.drop(\"trans_date_trans_time\", \"first\",\"last\",\"cc_num\",\"street\", \"city\", \"state\", \"zip\", \"job\", \"dob\",\"trans_num\")\n",
        "clean_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7li9d8a8Zqh"
      },
      "outputs": [],
      "source": [
        "# Changing the gender column to a integer format\n",
        "gender_df = clean_df.withColumn(\"gender\", f.when(clean_df[\"gender\"] == \"M\", 1).otherwise(0))\n",
        "gender_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sEvQUNS8Ztb"
      },
      "outputs": [],
      "source": [
        "# adding a indexer for the merchant names\n",
        "indexer = StringIndexer(inputCol=\"merchant\", outputCol=\"merchant_index\")\n",
        "df_indexed = indexer.fit(gender_df).transform(gender_df)\n",
        "\n",
        "# Using the pyspark package to convert the merchant column to a interger format\n",
        "encoder = OneHotEncoder(inputCols=[\"merchant_index\"], outputCols=[\"merchant_OHE\"])\n",
        "df_encoded = encoder.fit(df_indexed).transform(df_indexed)\n",
        "\n",
        "# Shwoing the results\n",
        "df_encoded.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YvWYvydw1z_"
      },
      "outputs": [],
      "source": [
        "# Repeating the process for the category index\n",
        "indexer_2 = StringIndexer(inputCol=\"category\", outputCol=\"category_index\")\n",
        "df_indexed_2 = indexer_2.fit(df_encoded).transform(df_encoded)\n",
        "\n",
        "# Repeating the process with the encoder for category column\n",
        "encoder_2 = OneHotEncoder(inputCols=[\"category_index\"], outputCols=[\"category_OHE\"])\n",
        "df_encoded_2 = encoder_2.fit(df_indexed_2).transform(df_indexed_2)\n",
        "\n",
        "# Showing the results\n",
        "df_encoded_2.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnV2eIOE0x7J"
      },
      "outputs": [],
      "source": [
        "# The final cleaing for for the model to use\n",
        "cleaned_df = df_encoded_2.drop(\"merchant\", \"category\",\"merchant_index\",\"category_index\")\n",
        "cleaned_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uG_oyR4tzaa5"
      },
      "outputs": [],
      "source": [
        "# Importing the correct packages for the learning process\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model 1**\n"
      ],
      "metadata": {
        "id": "jY5sxEn7qDsn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uv-YlG-a8xSf"
      },
      "outputs": [],
      "source": [
        "# Using the assembler to transform the the columns into a features column\n",
        "assembler = VectorAssembler(inputCols=[\"amt\", \"lat\", \"long\", \"city_pop\", \"unix_time\", \"merch_lat\", \"merch_long\", \"merchant_OHE\", \"category_OHE\"], outputCol=\"features\")\n",
        "df_assembled = assembler.transform(cleaned_df)\n",
        "\n",
        "# spliting the data into 80% training and 20% testing\n",
        "train_df, test_df = df_assembled.randomSplit([0.8, 0.2], seed=23)\n",
        "\n",
        "# using random forest to classify the fraud label\n",
        "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"is_fraud\", numTrees=10)\n",
        "\n",
        "# fitting the model\n",
        "rf_model = rf.fit(train_df)\n",
        "\n",
        "# adding a prediction function\n",
        "predictions = rf_model.transform(test_df)\n",
        "\n",
        "# evaluating the data to test for accuracy\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"is_fraud\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(f\"Model Accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjmYBMJGBicd"
      },
      "outputs": [],
      "source": [
        "# showing the data from our results\n",
        "predictions.select(\"_c0\", \"features\", \"is_fraud\", \"prediction\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8-RVk4oBiZQ"
      },
      "outputs": [],
      "source": [
        "# Calculating the value counts and printing the results\n",
        "valuecounts_data = predictions.groupBy(\"prediction\", \"is_fraud\").count()\n",
        "valuecounts_data.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **model 2**"
      ],
      "metadata": {
        "id": "7NepJZrdzc9O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u365MoDE4jdC"
      },
      "outputs": [],
      "source": [
        "# Dropping the original data in the is_fraud category\n",
        "cleaned_df_test = df_encoded_2.drop(\"is_fraud\",\"merchant\", \"category\",\"merchant_index\",\"category_index\")\n",
        "cleaned_df_test.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSp1usvG4jnd"
      },
      "outputs": [],
      "source": [
        "# adding a column that generates a number between 0 and 1 for each row, with 20% being 0 and 80% being 1\n",
        "from pyspark.sql.functions import rand, when\n",
        "df_with_fraud_test = cleaned_df_test.withColumn(\n",
        "    \"fraud_test\",\n",
        "    when(rand() <0.2, 1).otherwise(0)\n",
        ")\n",
        "\n",
        "# Showing the results\n",
        "df_with_fraud_test.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1GDts-G4jzv"
      },
      "outputs": [],
      "source": [
        "# Using the assembler to transform the the columns into a features column\n",
        "assembler_2 = VectorAssembler(inputCols=[\"amt\", \"lat\", \"long\", \"city_pop\", \"unix_time\", \"merch_lat\", \"merch_long\", \"merchant_OHE\", \"category_OHE\"], outputCol=\"features\")\n",
        "df_assembled_2 = assembler_2.transform(df_with_fraud_test)\n",
        "\n",
        "# spliting the data into 80% training and 20% testing\n",
        "train_df_2, test_df_2 = df_assembled_2.randomSplit([0.8, 0.2], seed=23)\n",
        "\n",
        "# using random forest to classify the fraud label\n",
        "rf_2 = RandomForestClassifier(featuresCol=\"features\", labelCol=\"fraud_test\", numTrees=10)\n",
        "\n",
        "# fitting the model\n",
        "rf_model_2 = rf_2.fit(train_df_2)\n",
        "\n",
        "# adding a prediction function\n",
        "predictions_2 = rf_model_2.transform(test_df_2)\n",
        "\n",
        "# evaluating the data to test for accuracy\n",
        "evaluator_2 = MulticlassClassificationEvaluator(labelCol=\"fraud_test\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy_2 = evaluator_2.evaluate(predictions_2)\n",
        "print(f\"Model Accuracy: {accuracy_2}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEGaiAaLV_yf"
      },
      "outputs": [],
      "source": [
        "# showing the data from our results\n",
        "predictions_2.select(\"_c0\", \"features\", \"fraud_test\", \"prediction\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrMKGCHXV_0o"
      },
      "outputs": [],
      "source": [
        "# Calculating the value counts and printing the results\n",
        "valuecounts_data_2 = predictions_2.groupBy(\"prediction\", \"fraud_test\").count()\n",
        "valuecounts_data_2.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **model 3**"
      ],
      "metadata": {
        "id": "saISG2cgqSJG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RgKT06bV_69"
      },
      "outputs": [],
      "source": [
        "## adding a column that generates a number between 0 and 1 for each row, with 50% being 0 and 50% being 1\n",
        "from pyspark.sql.functions import rand, when # Import the 'when' function\n",
        "df_with_fraud_test_3 = cleaned_df_test.withColumn(\n",
        "    \"fraud_test\",\n",
        "    when(rand() <0.5, 1).otherwise(0)\n",
        ")\n",
        "\n",
        "# Showing the results\n",
        "df_with_fraud_test_3.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the assembler to transform the the columns into a features column\n",
        "assembler_3 = VectorAssembler(inputCols=[\"amt\", \"lat\", \"long\", \"city_pop\", \"unix_time\", \"merch_lat\", \"merch_long\", \"merchant_OHE\", \"category_OHE\"], outputCol=\"features\")\n",
        "df_assembled_3 = assembler_3.transform(df_with_fraud_test_3)\n",
        "\n",
        "# spliting the data into 80% training and 20% testing\n",
        "train_df_3, test_df_3 = df_assembled_3.randomSplit([0.8, 0.2], seed=23)\n",
        "\n",
        "# using random forest to classify the fraud label\n",
        "rf_3 = RandomForestClassifier(featuresCol=\"features\", labelCol=\"fraud_test\", numTrees=10)\n",
        "\n",
        "# fitting the model\n",
        "rf_model_3 = rf_3.fit(train_df_3)\n",
        "\n",
        "# adding a prediction function\n",
        "predictions_3 = rf_model_3.transform(test_df_3)\n",
        "\n",
        "# evaluating the data to test for accuracy\n",
        "evaluator_3 = MulticlassClassificationEvaluator(labelCol=\"fraud_test\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy_3 = evaluator_3.evaluate(predictions_3)\n",
        "print(f\"Model Accuracy: {accuracy_3}\")"
      ],
      "metadata": {
        "id": "BnYL-c7Ng07k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# showing the data from our results\n",
        "predictions_3.select(\"_c0\", \"features\", \"fraud_test\", \"prediction\").show(truncate=False)"
      ],
      "metadata": {
        "id": "Ns3pMrIZh8J_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating the value counts and printing the results\n",
        "valuecounts_data_3 = predictions_3.groupBy(\"prediction\", \"fraud_test\").count()\n",
        "valuecounts_data_3.show()"
      ],
      "metadata": {
        "id": "53yyhzNLh8PC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **model 4**"
      ],
      "metadata": {
        "id": "D5cgPfUbqr-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# adding a column that generates a number between 0 and 1 for each row, with 50% being 0 and 50% being 1\n",
        "from pyspark.sql.functions import rand, when # Import the 'when' function\n",
        "df_with_fraud_test_4 = cleaned_df_test.withColumn(\n",
        "    \"fraud_test\",\n",
        "    when(rand() <0.5, 1).otherwise(0)\n",
        ")\n",
        "\n",
        "# Showing the results\n",
        "df_with_fraud_test_4.show()"
      ],
      "metadata": {
        "id": "slKVo72Hh8iW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the assembler to transform the the columns into a features column\n",
        "assembler_4 = VectorAssembler(inputCols=[\"amt\", \"lat\", \"long\", \"city_pop\", \"unix_time\", \"merch_lat\", \"merch_long\", \"merchant_OHE\", \"category_OHE\"], outputCol=\"features\")\n",
        "df_assembled_4 = assembler_4.transform(df_with_fraud_test_4)\n",
        "\n",
        "# spliting the data into 80% training and 20% testing and changing the seed\n",
        "train_df_4, test_df_4 = df_assembled_4.randomSplit([0.8, 0.2], seed=32)\n",
        "\n",
        "# using random forest to classify the fraud label\n",
        "rf_4 = RandomForestClassifier(featuresCol=\"features\", labelCol=\"fraud_test\", numTrees=10)\n",
        "\n",
        "# fitting the model\n",
        "rf_model_4 = rf_4.fit(train_df_4)\n",
        "\n",
        "# adding a prediction function\n",
        "predictions_4 = rf_model_4.transform(test_df_4)\n",
        "\n",
        "# evaluating the data to test for accuracy\n",
        "evaluator_4 = MulticlassClassificationEvaluator(labelCol=\"fraud_test\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy_4 = evaluator_4.evaluate(predictions_4)\n",
        "print(f\"Model Accuracy: {accuracy_4}\")"
      ],
      "metadata": {
        "id": "Af8-u5I8oZ7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# showing the data from our results\n",
        "predictions_4.select(\"_c0\", \"features\", \"fraud_test\", \"prediction\").show(truncate=False)"
      ],
      "metadata": {
        "id": "_9IonXDNoado"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating the value counts and printing the results\n",
        "valuecounts_data_4 = predictions_4.groupBy(\"prediction\", \"fraud_test\").count()\n",
        "valuecounts_data_4.show()"
      ],
      "metadata": {
        "id": "shr_xWN8qpMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing more pyspark packages\n",
        "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
        "\n",
        "# identifing the features and coverting into an array\n",
        "feature_names = [\"amt\", \"lat\", \"long\", \"city_pop\", \"unix_time\", \"merch_lat\", \"merch_long\", \"merchant_OHE\", \"category_OHE\"]\n",
        "feature_importances = rf_model_4.featureImportances\n",
        "importances = feature_importances.toArray()\n",
        "\n",
        "# converting into the correct language for pyspark to understand\n",
        "data = [(feature, float(importance)) for feature, importance in zip(feature_names, importances)]\n",
        "schema = StructType([\n",
        "    StructField(\"feature\", StringType(), True),\n",
        "    StructField(\"importance\", DoubleType(), True)\n",
        "])\n",
        "importances_df = spark.createDataFrame(data, schema)\n",
        "\n",
        "# Shwoing the results\n",
        "importances_df.show()"
      ],
      "metadata": {
        "id": "qNB76asUyBq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Collecting the importance values into a list\n",
        "importance_values = [0.03559887212631051, 0.03753287591376552, 0.022066089033708977, 0.02222002030989909,\n",
        "                     0.011923027363131824, 0.03455210984922371, 0.0, 0.0, 0.0]\n",
        "\n",
        "# Totaling the sum of importances\n",
        "total_importance = sum(importance_values)\n",
        "\n",
        "# Normalizing the importance values (divide each by the total)\n",
        "normalized_importances = [importance / total_importance for importance in importance_values]\n",
        "\n",
        "# Show normalized importances\n",
        "print(f\"Normalized Importances: {normalized_importances}\")\n"
      ],
      "metadata": {
        "id": "BLjyN30go2ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking sum to make sur it adds to 1\n",
        "sum(normalized_importances)\n"
      ],
      "metadata": {
        "id": "6_2Y58P1o2vV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing Spark session (if not already done)\n",
        "spark = SparkSession.builder.master(\"local\").appName(\"NormalizedFeatureImportance\").getOrCreate()\n",
        "\n",
        "# Original feature names\n",
        "feature_names = [\"amt\", \"lat\", \"long\", \"city_pop\", \"unix_time\", \"merch_lat\", \"merch_long\", \"merchant_OHE\", \"category_OHE\"]\n",
        "\n",
        "# Normalized importance values (calculated previously)\n",
        "normalized_importances = [0.216, 0.229, 0.135, 0.135, 0.073, 0.211, 0.0, 0.0, 0.0]\n",
        "\n",
        "# Combining feature names with normalized importances into tuples\n",
        "normalized_data = [(feature, importance) for feature, importance in zip(feature_names, normalized_importances)]\n",
        "\n",
        "# Defining the schema for the DataFrame\n",
        "schema = StructType([\n",
        "    StructField(\"feature\", StringType(), True),\n",
        "    StructField(\"normalized_importance\", DoubleType(), True)\n",
        "])\n",
        "\n",
        "# Creating the PySpark DataFrame from the combined data\n",
        "normalized_df = spark.createDataFrame(normalized_data, schema)\n",
        "\n",
        "# Show the resulting DataFrame\n",
        "normalized_df.show(truncate=False)"
      ],
      "metadata": {
        "id": "VcNweHl-o22i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing for visuals\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert the PySpark DataFrame to Pandas DataFrame for easy plotting\n",
        "pandas_df = normalized_df.toPandas()\n",
        "\n",
        "# Plotting the normalized feature importances using matplotlib\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(pandas_df['feature'], pandas_df['normalized_importance'], color='skyblue')\n",
        "plt.xlabel('Feature')\n",
        "plt.ylabel('Normalized Importance')\n",
        "plt.title('Normalized Feature Importance')\n",
        "plt.xticks(rotation=45, ha='right')  # Rotate feature names for readability\n",
        "plt.tight_layout()  # Adjust layout to avoid clipping\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ea-QLAh5o29D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Preparing the data for plotting\n",
        "fraud_test_values = [0, 1]  # Fraud categories (0 = not fraud, 1 = fraud)\n",
        "prediction_value = 0.0       # Only looking at prediction = 0.0\n",
        "\n",
        "# Counts for each fraud_test (is_fraud 0 and 1) under prediction = 1.0\n",
        "counts = np.array([368566, 1898])  # [not fraud, fraud]\n",
        "\n",
        "# Creating the stacked bar plot\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "# Bar width for each category\n",
        "bar_width = 0.3\n",
        "index = np.arange(len(fraud_test_values))  # Position of the bars on x-axis\n",
        "\n",
        "# Plotting the bars\n",
        "ax.bar(index, counts, bar_width, label=f\"Prediction: {prediction_value}\", color='skyblue')\n",
        "\n",
        "# Adding labels and title\n",
        "ax.set_xlabel('Actual Fraud Test', fontsize=12)\n",
        "ax.set_ylabel('Count', fontsize=12)\n",
        "ax.set_title(f'Stacked Bar Plot: Prediction = {prediction_value} vs Actual Fraud Test', fontsize=16)\n",
        "\n",
        "# Set x-axis labels\n",
        "ax.set_xticks(index)\n",
        "ax.set_xticklabels(['Not Fraud', 'Fraud'])\n",
        "\n",
        "# Adding the legend\n",
        "ax.legend()\n",
        "\n",
        "# Displaing the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6vjeNxpE0E5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Preparing the data for plotting\n",
        "prediction_values = [1.0, 0.0]\n",
        "fraud_test_values = [0, 1]\n",
        "\n",
        "# Arranging the counts in a 2x2 matrix for fraud_test vs prediction\n",
        "counts = np.array([[160136, 25310],  # fraud_test = 0 (Predicted: 1.0, 0.0)\n",
        "                   [25158, 159805]])  # fraud_test = 1 (Predicted: 0.0, 1.0)\n",
        "\n",
        "# Creating the stacked bar plot\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "# Bar width for each category\n",
        "bar_width = 0.3\n",
        "index = np.arange(len(fraud_test_values))  # Position of the bars on x-axis\n",
        "\n",
        "# Plotting the bars\n",
        "ax.bar(index, counts[0], bar_width, label=\"Predicted: fraud\", color='skyblue')  # Predicted fraud (fraud_test == 0)\n",
        "ax.bar(index, counts[1], bar_width, bottom=counts[0], label=\"Predicted: not fraud\", color='lightcoral')  # Predicted not fraud\n",
        "\n",
        "# Adding labels and title\n",
        "ax.set_xlabel('Actual Fraud Test', fontsize=12)\n",
        "ax.set_ylabel('Count', fontsize=12)\n",
        "ax.set_title('Stacked Bar Plot: Prediction vs Fraud Test', fontsize=16)\n",
        "\n",
        "# Setting x-axis labels\n",
        "ax.set_xticks(index)\n",
        "ax.set_xticklabels(['Not Fraud', 'Fraud'])\n",
        "\n",
        "# Adding the legend\n",
        "ax.legend()\n",
        "\n",
        "# Displaing the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "BDOK4p0g3dwt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}